{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa615551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a83856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('.../UrbanSound8K/metadata')\n",
    "meta_file = data_path/ 'UrbanSound8K.csv'\n",
    "df_meta = pd.read_csv(meta_file)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df_meta['relative_path'] = '/fold' + df_meta['fold'].astype(str) + '/' + df_meta['slice_file_name'].astype(str)\n",
    "df = df_meta[['relative_path', 'classID']]\n",
    "df = df[:1000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "idx = random.randint(0, len(df))\n",
    "audio_file, label  = str(data_path) + df.loc[idx, 'relative_path'], df_meta.loc[idx, 'class']\n",
    "print(\"True class: \\n\", label)\n",
    "audio_file = \"...UrbanSound8K/metadata\"+df.relative_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d252660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return sig, sr\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "        sig, sr = audio\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return audio\n",
    "        if (new_channel == 1):\n",
    "          # Convert from stereo to mono by selecting only the first channel\n",
    "          resig = sig[:1, :]\n",
    "        else:\n",
    "          # Convert from mono to stereo by duplicating the first channel\n",
    "          resig = torch.cat([sig, sig])\n",
    "        return ((resig, sr))\n",
    "     \n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "          # Nothing to do\n",
    "          return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "          # Resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000*max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            # Truncate\n",
    "            sig = sig[:, :max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            # Pad\n",
    "            pad_begin_len = random.randint(0, max_len-sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        # Convert to dB\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = r\".../urbanSound8k/audio\"+ df.loc[idx,\"relative_path\"]\n",
    "        class_id = self.df.loc[idx, 'classID'] \n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = SoundDS(df, data_path)\n",
    "# Split the dataset\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d34822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"dark_background\")\n",
    "def plot_spectrogram(batch_data, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(4,4, figsize=(16,10))\n",
    "    for idx, data in enumerate(batch_data):\n",
    "        row, col = idx//4, idx%4\n",
    "        ax = axs[row, col]\n",
    "        spec = data[0]\n",
    "        im = ax.imshow(spec, origin='lower', aspect=aspect)\n",
    "        if xmax:\n",
    "            ax.set_xlim((0, xmax))\n",
    "            fig.colorbar(im, ax=ax)\n",
    "            plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = df.classID.nunique()\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = {\n",
    "    \"base_model_name\": \"resnet50\",\n",
    "    \"pretrained\": False,\n",
    "}\n",
    "\n",
    "melspectrogram_parameters = {\n",
    "    \"n_mels\": 128,\n",
    "    \"fmin\": 20,\n",
    "    \"fmax\": 16000\n",
    "}\n",
    "print(DEVICE)\n",
    "weights_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e04287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier (nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)# girdileri 0 ve bir olarak ayırır # büyük verilerde çok daha hızlı çalışır \n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        self.clf = nn.Sequential(nn.Flatten(), \n",
    "        nn.Linear(64, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 10))\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.clf(x)\n",
    "\n",
    "        # Final output\n",
    "        return F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config: dict, weights_path):\n",
    "    model = ResNet(**config)\n",
    "    if weights_path:\n",
    "        checkpoint = torch.load(weights_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# myModel = get_model(model_config, weights_path)\n",
    "myModel = AudioClassifier().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3622287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                            steps_per_epoch=int(len(train_dl)),\n",
    "                                            epochs=num_epochs,\n",
    "                                            anneal_strategy='linear')\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        for i, data in enumerate(tqdm(train_dl)):\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "        \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d63f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=20\n",
    "train(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd2cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('anc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a70d153c6712dbf70d617484ad7b99edc521e857f7b8f17f3bc78df45ed0e9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
