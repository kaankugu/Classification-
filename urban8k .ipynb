{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\".../urbanSound8k/metadata/\")\n",
    "meta_file = data_path/ 'UrbanSound8K.csv'\n",
    "df_meta = pd.read_csv(meta_file)\n",
    "df_meta.head()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df_meta['relative_path'] = '/fold' + df_meta['fold'].astype(str) + '/' + df_meta['slice_file_name'].astype(str)\n",
    "df = df_meta[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2523a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0e8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "  \n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return sig, sr\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "        sig, sr = audio\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return audio\n",
    "        if (new_channel == 1):\n",
    "          resig = sig[:1, :]\n",
    "        else:\n",
    "          resig = torch.cat([sig, sig])\n",
    "        return ((resig, sr))\n",
    "     \n",
    " \n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "          return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000*max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            \n",
    "            sig = sig[:, :max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len-sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "  \n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "   \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = r\".../urbanSound8k/audio\"+ df.loc[idx,\"relative_path\"]\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = SoundDS(df, data_path)\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.7)\n",
    "num_val = round(num_items * 0.15)\n",
    "num_test = round(num_items * 0.15)\n",
    "\n",
    "train_ds, test_ds ,val_ds= random_split(myds, [num_train, num_test , num_val])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a20424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specturume(y, sr ):\n",
    "    n_fft = 2048 \n",
    "    ft = np.abs(librosa.stft(y[:n_fft], hop_length = n_fft+1))\n",
    "    spec = np.abs(librosa.stft(y, hop_length=512)) \n",
    "    spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024) \n",
    "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "    librosa.display.specshow(mel_spect, y_axis='mel', fmax=8000, x_axis='time'); \n",
    "    plt.title('Mel SpektrogramÄ±'); \n",
    "    plt.colorbar(format='%+2.0f dB');\n",
    "    return (mel_spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"dark_background\")\n",
    "def plot_spectrogram(batch_data, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(4,4, figsize=(16,10))\n",
    "    for idx, data in enumerate(batch_data):\n",
    "        row, col = idx//4, idx%4\n",
    "        ax = axs[row, col]\n",
    "        spec = data[0]\n",
    "        im = ax.imshow(spec, origin='lower', aspect=aspect)\n",
    "        if xmax:\n",
    "            ax.set_xlim((0, xmax))\n",
    "            fig.colorbar(im, ax=ax)\n",
    "            plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = df.classID.nunique()\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = {\n",
    "    \"base_model_name\": \"resnet50\",\n",
    "    \"pretrained\": False,\n",
    "}\n",
    "\n",
    "melspectrogram_parameters = {\n",
    "    \"n_mels\": 128,\n",
    "    \"fmin\": 20,\n",
    "    \"fmax\": 16000\n",
    "}\n",
    "\n",
    "weights_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class AudioClassifier (nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        conv_layers += [self.conv5, self.relu5, self.bn5]\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        conv_layers += [self.conv6, self.relu6, self.bn6]\n",
    "\n",
    "     \n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(in_features=128,out_features=256),\n",
    "            nn.Linear(in_features=256,out_features=512),\n",
    "            nn.Linear(in_features=512,out_features=1024),\n",
    "            nn.Linear(in_features=1024,out_features=10))\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = AudioClassifier().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                            steps_per_epoch=int(len(train_dl)),\n",
    "                                            epochs=10,\n",
    "                                            anneal_strategy='linear')\n",
    "    running_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(tqdm(train_dl)):\n",
    "        inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction / total_prediction\n",
    "    return avg_loss,acc,model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62355e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "model = AudioClassifier()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "running_loss = 0.0\n",
    "running_loss = 0.0\n",
    "vcorrect_prediction = 0\n",
    "vtotal_prediction = 0\n",
    " \n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "model = myModel\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss, acc_train,model = train(myModel, train_dl)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            vinputs, vlabels = vdata[0].to(DEVICE), vdata[1].to(DEVICE)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            _ , prediction = torch.max(voutputs,1)\n",
    "            vcorrect_prediction += (prediction == vlabels).sum().item()\n",
    "            vtotal_prediction += prediction.shape[0]\n",
    "\n",
    "    acc_val = vcorrect_prediction/vtotal_prediction\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training Loss' : avg_loss, 'Validation Loss' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    print(f\"Validation accuricy :{acc_val}\") \n",
    "    print(f\"Training accuricy :{acc_train}\\n\") \n",
    "    \n",
    "    \n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def inference (model, val_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    f1_scor  =[]\n",
    "    Confmat = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_dl):\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "            f1 = F1Score(num_classes=10)\n",
    "            f1_scor.append(f1(outputs, labels)) \n",
    "            \n",
    "            confmat = ConfusionMatrix(num_classes=10)\n",
    "            Confmat.append(confmat(outputs, labels))\n",
    "\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc}, Total items: {total_prediction} \\nF1 Score : {max(f1_scor)}')\n",
    "    print(f\"Confusition Matrix : {Confmat}\")\n",
    "\n",
    "inference(myModel, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('anc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a70d153c6712dbf70d617484ad7b99edc521e857f7b8f17f3bc78df45ed0e9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
